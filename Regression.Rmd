
**Setup**
```{r, results='hide'}
# Load Libraries
library(caret)
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Split data into training and testing 70%/30%
df <- read.csv("/Users/sonakrishnan/Desktop/STAT_432/scaled_used_cars.csv")
trainselect <- sample(nrow(df), 0.7 * nrow(df))
training <- df[trainselect,]
testing <- df[-trainselect,]
```

**OLS Model**

```{r, results='hide'}

# Selecting OLS model

# Filter Out Categorical Variables that cannot be turned numeric 

filteredtraining <- subset(training, select=-c(brand, model, engine))
filteredtesting <- subset(testing, select=-c(brand, model, engine))

# Initial Models
fullmodel <- lm(price ~ ., data = filteredtraining)
nullmodel <- lm(price ~ 1, data = filteredtraining)

# Forward Selection
forwardmodel <- step(nullmodel, scope = list(lower = nullmodel, upper = fullmodel), direction = "forward")

# Backward Selection
backwardmodel <- step(fullmodel, direction = "backward")

# Stepwise Selection
stepmodel <- step(nullmodel, direction = "both")

```

```{r}

# Full Model
fullpredictions <- predict(fullmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - fullpredictions)^2)
cat("\nMSE: ", mse)
cat("\nR-Squared: ", summary(fullmodel)$r.squared)

# Forward Model
predictions <- predict(forwardmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - predictions)^2)
cat("\nMSE: ", mse)
cat("\nR-Squared: ", summary(forwardmodel)$r.squared)

# Backward Model
predictions <- predict(backwardmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - predictions)^2)
cat("\nMSE: ", mse)
cat("\nR-Squared: ", summary(backwardmodel)$r.squared)

# Step Model
predictions <- predict(stepmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - predictions)^2)
cat("\nMSE: ", mse)
cat("\nR-Squared: ", summary(stepmodel)$r.squared)


```
Our best OLS model was the Full Model, which had an MSE of 0.7474024 and an $R^2$ of 0.1550012. This means that about 15.5% of the variance in the price of used cars can be explained by the full OLS model. 


```{r}
dfplot <- data.frame(x = filteredtesting$price, y = fullpredictions)
ggplot(dfplot, aes(x = x, y = y)) + geom_point(size=1) + geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") + labs(title = "OLS Full Model", x = "Actual Price", y = "Predicted Price") + xlim(0, 10) + ylim(-1, 2)
```



**KNN Model**



```{r, results='hide'}

# KNN Model

trainControl <- trainControl(method = "cv", number = 10)
knnmodel <- train(price ~ ., data = filteredtraining, method = "knn", trControl = trainControl)
knnmodel
```

We use the default distance metric for the caret library, Euclidean distance. As for the categorical variables, we used the same approach Samruddhi, K., and R. Ashok Kumar [2] took in their attempt for a KNN model and removed predictors that could not be turned into a numeric form. 




```{r}

predictions <- predict(knnmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - predictions)^2)
cat("\nMSE: ", mse)

rsquared <- 1 - (sum((filteredtesting$price - predictions)^2) / sum((filteredtesting$price - mean(filteredtesting$price))^2))
cat("\nR-Squared: ", rsquared)
```

Our KNN model has an MSE of 0.7402953 and an $R^2$ value of 0.1764681. This means that about 17.6% of the variance in the price of used cars can be explained by the KNN model.

```{r}
# New model with higher tune length
set.seed(123)
knnmodel <- train(price ~ ., data = filteredtraining, method = "knn", trControl = trainControl, tuneLength = 5)
predictions <- predict(knnmodel, newdata = filteredtesting)
mse <- mean((filteredtesting$price - predictions)^2)
cat("\nMSE: ", mse)

rsquared <- 1 - (sum((filteredtesting$price - predictions)^2) / sum((filteredtesting$price - mean(filteredtesting$price))^2))
cat("\nR-Squared: ", rsquared)
```

By increasing the tune length we were able to analyze more values of k, and we found that the optimal k value was 11 as opposed to 9 in the last model. This resulted in a lower MSE, 0.7301051, and a higher $R^2$, 0.1878041. This means that about 18.8% of the variance in the price of used cars can be explained by the new KNN model. 


```{r}
dfplot <- data.frame(x = filteredtesting$price, y = predictions)
ggplot(dfplot, aes(x = x, y = y)) + geom_point(size=1) + geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") + labs(title = "KNN Model", x = "Actual Price", y = "Predicted Price") + xlim(0, 10) + ylim(0, 4.5)
```

#Load Libraries
```{r}
#load libraries
library(randomForest)
library(caret)
library(Metrics)
library(e1071)
library(ggplot2)
library(xgboost)
```

#Load and split data into testing and training
```{r}
data <- read.csv("/Users/sonakrishnan/Desktop/STAT_432/scaled_used_cars.csv")

# Remove non-numeric columns
X <- data[, !(names(data) %in% c("price", "brand", "model", "engine"))]
y <- data$price

set.seed(123)
trainIndex <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[trainIndex, ]
y_train <- y[trainIndex]
X_test <- X[-trainIndex, ]
y_test <- y[-trainIndex]
```

#Random Forest

```{r}
# Define the tuning grid
rf_grid <- expand.grid(
  mtry = c(2, floor(sqrt(ncol(X_train))), floor(ncol(X_train)/3)),
  nodesize = c(5, 10, 20)
)

tuning_results <- data.frame(mtry = numeric(), nodesize = numeric(), RMSE = numeric())

# Iterate through all parameter combinations
for (i in 1:nrow(rf_grid)) {
  set.seed(123)
  rf_temp <- randomForest(
    x = X_train,
    y = y_train,
    mtry = rf_grid$mtry[i],
    nodesize = rf_grid$nodesize[i],
    ntree = 200  
  )
  
  y_pred_temp <- predict(rf_temp, X_test)
  
  rmse_temp <- rmse(y_test, y_pred_temp)
  
  tuning_results <- rbind(tuning_results, 
                          data.frame(
                            mtry = rf_grid$mtry[i], 
                            nodesize = rf_grid$nodesize[i], 
                            RMSE = rmse_temp
                          ))
}

# Find the best combination (lowest RMSE)
best_params <- tuning_results[which.min(tuning_results$RMSE), ]
print(best_params)
```

```{r}
# Train Random Forest model with optimal hyperparameters
set.seed(123)
rf_final <- randomForest(
  x = X_train, 
  y = y_train, 
  mtry = best_params$mtry, 
  nodesize = best_params$nodesize, 
  ntree = 200  # Fixed number of trees
)

# Predict on the test set
rf_pred_final <- predict(rf_final, X_test)

# Evaluate performance
rf_final_metrics <- data.frame(
  Model = "Random Forest (Tuned)",
  RMSE = rmse(y_test, rf_pred_final),
  MAE = mae(y_test, rf_pred_final),
  R_Squared = R2(rf_pred_final, y_test)
)

rf_final_metrics
```
The parameters tuned were mtry and nodesize. 
The tuning process:
A grid was constructed with combinations of mtry and node size. A loop iterated over each combination in the grid. For each combination, a random forest model was trained with the specified parameters and predictions were made on the test set. Performance was evaluated with RMSE, MAE, and $R^2$. The combination with the lowest RMSE on the test set was chosen as the optimal parameters. The Random Forest model was retrained on the entire training dataset using the optimal parameters. 
The train function systematically evaluated each combination of hyperparameters within the grid. For each combination:
A Random Forest model was trained on the training dataset (X_train, y_train) with the specified mtry and nodesize.
The model's performance was assessed using cross-validated RMSE.

Metrics Used:
RMSE
  * Advantages:
      * Penalizes large errors more than small ones
      * Interpretable in the target variable's units
  * Disadvantages:
      * Sensitive to outliers
      * Does not give the full picture of prediction accuracy
MAE
  * Advantages
      * Robust to outliers
      * Straightforward interpretation of average prediction error
  * Disadvantages
      * Treats all errors equally
$R^2$
  * Advantages
      * Explain the proportion of variance captured by the model
      * Useful for comparing models with different features
  * Disadvantages
      * Can give misleading values for non-linear models
      * A high $R^2$ does not necessarily imply good predictive performance

```{r}
# Create a dataframe for plotting
rf_plot_data <- data.frame(
  Actual = y_test,
  Predicted = rf_pred_final
)

# Plot Actual vs Predicted
ggplot(rf_plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "forestgreen", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Random Forest: Actual vs Predicted Car Prices",
    x = "Actual Prices",
    y = "Predicted Prices"
  )

```

#Support Vector Regression (SVR)

```{r}
# Define training control with 5-fold cross-validation
svr_control <- trainControl(method = "cv", number = 5, verboseIter = FALSE)

# Define the grid of hyperparameters
svr_grid <- expand.grid(
  C = c(0.1, 1, 10, 100),
  sigma = c(0.001, 0.01, 0.1, 1)
)

# Perform grid search with cross-validation
set.seed(123)
svr_tuned <- train(
  x = X_train,
  y = y_train,
  method = "svmRadial",
  trControl = svr_control,
  tuneGrid = svr_grid,
  preProcess = c("center", "scale")
)

# Best hyperparameters
svr_best_params <- svr_tuned$bestTune
svr_best_params
```

```{r}
# Predict on test data using the tuned SVR model
svr_pred_final <- predict(svr_tuned, X_test)

# Evaluate performance
svr_final_metrics <- data.frame(
  Model = "SVR (Tuned)",
  RMSE = rmse(y_test, svr_pred_final),
  MAE = mae(y_test, svr_pred_final),
  R_Squared = R2(svr_pred_final, y_test)
)

svr_final_metrics
```
The parameters tuned were C and Sigma. Larger C values aim for a smaller margin hyperplane, minimizing training errors but risking overfitting. Smaller sigma values make the kernel function more localized, capturing finer details but increasing the risk of overfitting.
Tuning Process:
A grid comprising various combinations of C and sigma values was created to explore different levels of regularization and kernel influence. A 5-fold cross-validation approach was utilized to ensure the model's robustness across different data partitions.The train function evaluated each combination within the grid, training an SVR model with the specified C and sigma values and assessing performance via cross-validated RMSE. The combination with the lowest RMSE was selected as the optimal set of hyperparameters for the SVR model.
Metrics Used:
  * RMSE
  * MAE
  * $R^2%

```{r}
# Plot RMSE across different hyperparameters
ggplot(svr_tuned$results, aes(x = factor(C), y = RMSE, fill = factor(sigma))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "SVR Tuning: C vs RMSE",
       x = "C",
       y = "RMSE",
       fill = "sigma") +
  theme_minimal()
```


```{r}
# Create a dataframe for plotting
svr_plot_data <- data.frame(
  Actual = y_test,
  Predicted = svr_pred_final
)

# Plot Actual vs Predicted
ggplot(svr_plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "steelblue", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "SVR: Actual vs Predicted Car Prices",
    x = "Actual Prices",
    y = "Predicted Prices"
  )

```

#XGBoost Regression

```{r}
# Convert data to matrix format for XGBoost
X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)
```

```{r}
# Define training control with 5-fold cross-validation
xgb_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  allowParallel = TRUE
)
library(doParallel)
cl <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
# Define the grid of hyperparameters
xgb_grid <- expand.grid(
  nrounds = c(100, 200, 300),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3),
  gamma = c(0, 1, 5),
  colsample_bytree = c(0.6, 0.8, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.6, 0.8, 1)
)

# Train the XGBoost model
set.seed(123)  
xgb_model <- train(
  x = X_train_matrix,
  y = y_train,
  method = "xgbTree",
  trControl = xgb_control,
  tuneGrid = xgb_grid,
  verbose = FALSE,
)
stopCluster(cl)
registerDoSEQ()

print(xgb_model$bestTune)
```

```{r}
# Make predictions on the test set
xgb_pred_final <- predict(xgb_model, newdata = X_test_matrix)

# Evaluate performance
xgb_final_metrics <- data.frame(
  Model = "XGBoost",
  RMSE = rmse(y_test, xgb_pred_final),
  MAE = mae(y_test, xgb_pred_final),
  R_Squared = R2(xgb_pred_final, y_test)
)

xgb_final_metrics
```
The parameters tuned were nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample. 
Tuning Process:
A comprehensive grid encompassing a range of values for each hyperparameter was established to explore various model complexities and regularization strengths. Utilized a 5-fold cross-validation approach to ensure robust performance evaluation across different data partitions. The train function systematically evaluated each hyperparameter combination, training an XGBoost model with the specified parameters and assessing performance via cross-validated RMSE. The combination with the lowest RMSE was selected as the optimal set of hyperparameters for the XGBoost model.

Metrics Used:
  * RMSE
  * MAE
  * $R^2%

```{r}
# Create a dataframe for plotting
xgb_plot_data <- data.frame(
  Actual = y_test,
  Predicted = xgb_pred_final
)

# Plot Actual vs Predicted
ggplot(xgb_plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "darkorange", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "XGBoost: Actual vs Predicted Car Prices",
    x = "Actual Prices",
    y = "Predicted Prices"
  )
```

```{r}
# Compute feature importance
xgb_importance <- xgb.importance(model = xgb_model$finalModel)

# Plot top 10 feature importance
xgb.plot.importance(xgb_importance, top_n = 10, measure = "Gain", main = "Feature Importance")
```

#Compare Models

```{r}
# Compile all model performances
summary_results <- rbind(
  rf_final_metrics,
  svr_final_metrics,
  xgb_final_metrics
)

# Display the summary table
print(summary_results)
```
```{r}
# Load libraries
library(ggplot2)
library(caret)
library(dplyr)

# Example data: Random Forest and XGBoost feature importance
rf_importance <- data.frame(
  Feature = c("milage", "engine_displacement", "horsepower", "model_year", "accident"),
  Importance = c(12.91, 7.19, 9.57, 12.62, 6.12)
)

xgb_importance <- data.frame(
  Feature = c("milage", "engine_displacement", "horsepower", "model_year", "accident"),
  Importance = c(0.332635, 0.308950, 0.168780, 0.139647, 0.004082)
)

# Normalize XGBoost importance
xgb_importance$Importance <- (xgb_importance$Importance / max(xgb_importance$Importance)) * max(rf_importance$Importance)

# Calculate Permutation Importance for SVR
set.seed(123)
perm_importance <- function(model, data, target, metric = "RMSE") {
  baseline <- postResample(predict(model, data), target)[[metric]]
  sapply(names(data), function(feature) {
    permuted_data <- data
    permuted_data[[feature]] <- sample(permuted_data[[feature]])
    permuted_score <- postResample(predict(model, permuted_data), target)[[metric]]
    return(abs(baseline - permuted_score))  # Importance is the performance drop
  })
}

# Assuming you have an SVR model trained and your test set
svr_importance_scores <- perm_importance(svr_model, X_test, y_test)

# Convert to dataframe
svr_importance <- data.frame(
  Feature = names(svr_importance_scores),
  Importance = svr_importance_scores
)

# Normalize SVR importance
svr_importance$Importance <- (svr_importance$Importance / max(svr_importance$Importance)) * max(rf_importance$Importance)

# Combine all importance scores
importance_combined <- rf_importance %>%
  rename(Random_Forest = Importance) %>%
  left_join(xgb_importance %>% rename(XGBoost = Importance), by = "Feature") %>%
  left_join(svr_importance %>% rename(SVR = Importance), by = "Feature")

# Reshape for ggplot
importance_long <- pivot_longer(importance_combined, cols = c("Random_Forest", "XGBoost", "SVR"),
                                names_to = "Model", values_to = "Importance")

# Plot
ggplot(importance_long, aes(x = Feature, y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  labs(
    title = "Feature Importance Comparison Across Models",
    x = "Feature",
    y = "Normalized Importance Score",
    fill = "Model"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# Permutation importance function
perm_importance <- function(model, data, target, metric = "RMSE") {
  baseline <- postResample(predict(model, data), target)[[metric]]
  sapply(names(data), function(feature) {
    permuted_data <- data
    permuted_data[[feature]] <- sample(permuted_data[[feature]])
    permuted_score <- postResample(predict(model, permuted_data), target)[[metric]]
    return(abs(baseline - permuted_score))
  })
}

# Compute KNN importance
knn_importance_scores <- perm_importance(knnmodel, filteredtesting[, -which(names(filteredtesting) == "price")], filteredtesting$price)
knn_importance <- data.frame(
  Feature = names(knn_importance_scores),
  Importance = knn_importance_scores / max(knn_importance_scores) * max(rf_importance$Importance)  # Normalize
)

svr_importance_scores <- perm_importance(svr_model, filteredtesting[, -which(names(filteredtesting) == "price")], filteredtesting$price)

# Normalize SVR importance
svr_importance <- data.frame(
  Feature = names(svr_importance_scores),
  Importance = svr_importance_scores / max(svr_importance_scores) * max(rf_importance$Importance)  # Normalize
)
```

```{r}
# Combine all importance scores
importance_combined <- rf_importance %>%
  rename(Random_Forest = Importance) %>%
  left_join(xgb_importance %>% rename(XGBoost = Importance), by = "Feature") %>%
  left_join(knn_importance %>% rename(KNN = Importance), by = "Feature") %>%
  left_join(svr_importance %>% rename(SVR = Importance), by = "Feature")

# Reshape for ggplot
importance_long <- pivot_longer(importance_combined, cols = c("Random_Forest", "XGBoost", "KNN", "SVR"),
                                names_to = "Model", values_to = "Importance")

# Plot
ggplot(importance_long, aes(x = Feature, y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  labs(
    title = "Feature Importance Comparison Across Models",
    x = "Feature",
    y = "Normalized Importance Score",
    fill = "Model"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))  # Add space for visibility

```

#Open Ended 

```{r}
# Select three cars from the dataset
selected_cars <- data[1:3, ]  # Example: Selecting the first three cars
selected_cars$milage <- 0  # Set mileage to 0 for new cars
selected_cars$accident <- 0  # Set accident history to "None"
selected_cars$model_year <- c(2023, 2024, 2021)  # Adjust to actual release years

# Predict original prices
predicted_new_prices <- predict(rf_final, newdata = selected_cars)

# Compare with actual release prices (manually gathered)
actual_new_prices <- c(30000, 35000, 45000)  # Example: Online research for release prices

# Combine results
comparison <- data.frame(
  Car = c("Car 1", "Car 2", "Car 3"),
  Predicted_Price = predicted_new_prices,
  Actual_Release_Price = actual_new_prices,
  Difference = actual_new_prices - predicted_new_prices
)
print(comparison)

```

```{r}
data1 <- read.csv("/Users/sonakrishnan/Desktop/STAT_432/original_used_cars.csv")

```

```{r}
# Mean and standard deviation of the price column
price_mean <- mean(data1[["price"]], na.rm = TRUE)
price_sd <- sd(data1[["price"]], na.rm = TRUE)

# Predicted standardized prices
predicted_standardized_prices <- c(-0.16298310, 0.09293002, 0.03208888)

# Reverse standardization to get real prices
predicted_real_prices <- (predicted_standardized_prices * price_sd) + price_mean

# Combine results into a data frame
comparison <- data.frame(
  Car = c("Car 1", "Car 2", "Car 3"),
  Predicted_Price = predicted_real_prices,
  Actual_Release_Price = c(30000, 35000, 45000),  # Replace with actual values
  Difference = c(30000, 35000, 45000) - predicted_real_prices
)

# Print the comparison
print(comparison)

```







